{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3b77f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from data import final_features, winsorize_variables\n",
    "pd.set_option('display.max_columns',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c1d2e",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "In this section preprocessing the created features will be applied in order to quit NA's, treat outliers and select most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f31a3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessor(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    winsorize_variables,\n",
    "    final_features,\n",
    "    n_estimators: int=100,\n",
    "    max_features: int=None,\n",
    "    random_state: int=102\n",
    "    \n",
    "):\n",
    "    steps = [\n",
    "        ('zero_imputer', SklearnTransformerWrapper(\n",
    "            SimpleImputer(\n",
    "                missing_values=np.nan, strategy='constant', fill_value=0\n",
    "                ), variables=final_features))\n",
    "        ,('max_winsorizer', Winsorizer(\n",
    "            variables=winsorize_variables, capping_method='iqr', tail='right', \n",
    "            fold=3))\n",
    "    ]\n",
    "    # Running initial pipeline\n",
    "    initial_pipeline = Pipeline(steps, verbose=True)\n",
    "    initial_pipeline.fit(X)\n",
    "    \n",
    "    X_preprocessed = initial_pipeline.transform(X)\n",
    "    \n",
    "    # Fetaure selector\n",
    "    feature_selector = SelectFromModel(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=n_estimators, random_state=random_state\n",
    "            ),\n",
    "            max_features=max_features, threshold=0\n",
    "            )\n",
    "    feature_selector.fit(X_preprocessed, y)\n",
    "    column_names = feature_selector.get_feature_names_out().tolist()\n",
    "    \n",
    "    # Defining final preprocess pipeline\n",
    "    steps = [\n",
    "            ('preprocessor', initial_pipeline),\n",
    "            ('selector', feature_selector)\n",
    "        ]\n",
    "\n",
    "    # Running preprocess pipeline\n",
    "    preprocessor = Pipeline(steps, verbose=True)\n",
    "    \n",
    "    return preprocessor, column_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd623b",
   "metadata": {},
   "source": [
    "###### Define some parameters to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f10fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/features_preprocessed.csv')\n",
    "time_split = True\n",
    "test_size = 0.2\n",
    "random_state = 102\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455e216",
   "metadata": {},
   "source": [
    "###### I will implement an Out Of Time train/test split, which is more suitable for this type of problem. In this approach, we train our model on data from a specific time period and subsequently evaluate its performance on new data. Therefore, an out-of-time partitioning strategy is a more effective approach for handling time-dependent scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb7a3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_split:\n",
    "    data['APPLICATION_DATETIME'] = pd.to_datetime(data['APPLICATION_DATETIME'])\n",
    "    data = data.sort_values(by='APPLICATION_DATETIME', ascending=True)\n",
    "    train, test = data.iloc[:int(data.shape[0] * (1 - test_size)), :], \\\n",
    "                        data.iloc[int(data.shape[0] * (1 - test_size)):, :]\n",
    "else:\n",
    "    train, test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    \n",
    "y_train = train[target]\n",
    "X_train = train[final_features]\n",
    "\n",
    "y_test = test[target]\n",
    "X_test = test[final_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00991ce3",
   "metadata": {},
   "source": [
    "###### Create the preprocessor with the function i defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90123aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing zero_imputer, total=   0.1s\n",
      "[Pipeline] .... (step 2 of 2) Processing max_winsorizer, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# Creating preprocess pipeline\n",
    "preprocessor, columns_names = get_preprocessor(\n",
    "            X=X_train, y=y_train, winsorize_variables=winsorize_variables,\n",
    "            final_features=final_features, random_state=random_state,\n",
    "            max_features=75\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c24af",
   "metadata": {},
   "source": [
    "###### Save the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a37281e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/preprocessor.joblib.dat']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(preprocessor, 'models/preprocessor.joblib.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941bcd1",
   "metadata": {},
   "source": [
    "###### Preprocess the train/test data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "924e30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train set\n",
    "train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), columns=columns_names\n",
    "    )\n",
    "train_preprocessed[target] = y_train.values\n",
    "train_preprocessed.to_csv('data/train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f73142cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train set\n",
    "test_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_test), columns=columns_names\n",
    "    )\n",
    "test_preprocessed[target] = y_test.values\n",
    "test_preprocessed.to_csv('data/test_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
